{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxIquH6GK7kt"
      },
      "source": [
        "# Домашнее задание. Производная, градиент и градиентный спуск\n",
        "\n",
        "В этом домашнем задании вам предстоит поработать с понятием производной и градиента, а также написать градиентный спуск и его вариации.\n",
        "\n",
        "__Напоминание:__\n",
        "Производной функции $f$ в точке $x$ называется выражение\n",
        "\n",
        "$$\\lim_{h→0}\\frac{f(x+h)−f(x)}{h}$$\n",
        "Или, что то же самое,\n",
        "$$\\lim_{x→x_0}\\frac{f(x)−f(x_0)}{x-x_0}$$\n",
        "\n",
        "Если такой предел существует, то и производная существует (и равна этому пределу)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIzAFQsnvod2"
      },
      "source": [
        "from copy import copy, deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6JiQgamvod9"
      },
      "source": [
        "## Задание 1 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y38bKm9Tvod-"
      },
      "source": [
        "Какие из перечисленных функций имеют производную в нуле $(x_0 = 0)$?\n",
        "\n",
        "1) $f(x) = |x|^2$\n",
        "\n",
        "2) $f(x) = \\frac{sin(x)}{x}$\n",
        "\n",
        "3) $f(x) = |x|$\n",
        "\n",
        "4) $f(x) =\n",
        "     \\begin{cases}\n",
        "       x^2, &\\text{$x \\ne 0$}; \\\\\n",
        "       0, &\\text{$x = 0$}\n",
        "     \\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfdVpqeTvoeA"
      },
      "source": [
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc4hJhHuvoeC"
      },
      "source": [
        "## Задание 2 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_MN9myzvoeD"
      },
      "source": [
        "Посчитайте производную $f(x)=x^x$ в точке $x_0 = e$\n",
        "\n",
        "Ответ округлите до одного знака после запятой.\n",
        "\n",
        "*Указание*. Представьте функцию $f(x)$ как $e^{g(x)}$ для некоторой $g$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBcFA6ErvoeF"
      },
      "source": [
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsJ_W3VXvoeH"
      },
      "source": [
        "## Задание 3 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj-eLSJyvoeI"
      },
      "source": [
        "Вычислите производную $f(x)=tg(x)⋅\\ln(\\cos(x^2)+1)$, в точке $x_0 = 0$. Ответ округлите до двух знаков после запятой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctY73lF1voeK"
      },
      "source": [
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHjndqfCvoeL"
      },
      "source": [
        "## Задание 4 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve1wPT74voeN"
      },
      "source": [
        "​Ваше задание --- написать python-функцию, которая в качестве аргумента принимает:\n",
        "\n",
        "числовую функцию $f$, у которой необходимо вычислить производную\n",
        "число $\\varepsilon$ --- его необходимо использовать в качестве \"малого шага\" для приближённого вычисления производной.\n",
        "Функция должна в свою очередь возвращать числовую функцию $f'$, равную производной функции $f$.\n",
        "\n",
        "Однако не подумайте, что вас просят написать что-то, что будет вычислять эту самую производную аналитически. Производную следует вычислять по формуле $$f'(x)\\approx \\frac{f(x+\\varepsilon) - f(x)}{\\varepsilon}.$$\n",
        "​"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYuZF08nvoeO"
      },
      "source": [
        "def numerical_derivative_1d(func, epsilon):\n",
        "    \"\"\"\n",
        "    Функция для приближённого вычисления производной функции одной переменной.\n",
        "    :param func: float -> float — произвольная дифференцируемая функция\n",
        "    :param epsilon: float — максимальная величина приращения по оси Ох\n",
        "    :return: другая функция, которая приближённо вычисляет производную в точке\n",
        "    \"\"\"\n",
        "    def deriv_func(x):\n",
        "        \"\"\"\n",
        "        :param x: float — точка, в которой нужно вычислить производную\n",
        "        :return: приближённое значение производной в этой точке\n",
        "        \"\"\"\n",
        "        return (func(x + epsilon) - func(x)) / epsilon\n",
        "\n",
        "    return deriv_func"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogo1thL-voeT",
        "outputId": "73630bb5-b077-4a40-bfad-2e8ba6f7d34e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Проверьте себя!\n",
        "def polynom_to_prime(x):\n",
        "    return 20 * x**5 + x**3 - 5 * x**2 + 2 * x + 2.0\n",
        "\n",
        "\n",
        "def primed_poly(x):\n",
        "    return 100 * x**4 + 3 * x**2 -10 * x + 2.0\n",
        "\n",
        "\n",
        "approx_deriv = numerical_derivative_1d(polynom_to_prime, 1e-5)\n",
        "\n",
        "grid = np.linspace(-2, 2, 100)\n",
        "right_flag = True\n",
        "tol = 0.05\n",
        "debug_print = []\n",
        "\n",
        "for x in grid:\n",
        "    estimation_error = abs(primed_poly(x) - approx_deriv(x))\n",
        "    if estimation_error > tol:\n",
        "        debug_print.append((estimation_error, primed_poly(x), approx_deriv(x)))\n",
        "        right_flag = False\n",
        "\n",
        "if not right_flag:\n",
        "    print(\"Что-то не то...\")\n",
        "    print(debug_print)\n",
        "    plt.plot(grid, primed_poly(grid), label=\"Истинная производная\")\n",
        "    plt.plot(grid, approx_deriv(grid), label=\"Численное приближение\")\n",
        "    plt.legend()\n",
        "\n",
        "print(str(right_flag))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91UYo01wvoea"
      },
      "source": [
        "## Задание 5\n",
        "\n",
        "В этом задании Вы должны реализовать функцию `grad_descent_v1` для нахождения минимума функции с помощью градиентного спуска.\n",
        "\n",
        "**Вход фукнции**.\n",
        "* Функция `func`, которую нужно оптимизировать\n",
        "* Ее производная `deriv`. В данном пункте производная вам дана, вычислять ее самостоятельно не нужно. (*)\n",
        "* Начальная точка `start`.\n",
        "\n",
        "**Выход функции** -- точка локального минимума.\n",
        "\n",
        "Для вашего удобства мы написали функцию для отрисовки траектории градиентного спуска.\n",
        "\n",
        "В реализации градиентного спуска можете предполагать, что на вход подаются функции с единственным, глобальным минимумом. Перед тем, как писать код, ответьте себе на следующие вопросы:\n",
        "\n",
        "* Как понять, что пора остановиться? Это может зависеть от градиента или расстояния между двумя соседними шагами алгоритма, так и от числа уже выполненных итераций.\n",
        "* Как правильно менять величину шага (`learning rate`) от итерации к итерации?\n",
        "\n",
        "В этом пункте гарантируется, что существует решение, использующее обычный градиентный спуск с фиксированным learning rate и наперёд заданным количеством итераций. Если вы хотите поэкспериментировать с возможными решениями, вот несколько советов по реализации градиентного спуска (которые, тем не менее, не обязательны для решения этой задачи):\n",
        "\n",
        "* Сделайте шаг обучения не константным, а зависящим от номера итерации. Неплохая эвристика --- домножать `lr` на $ \\frac{1}{ \\sqrt{iteration}}$.\n",
        "\n",
        "* В функциях могут после первого же шага градиентного спуска появляться очень большие значения. Для того, чтобы не вылезать за пределы отрезка, на котором ищется минимум, после каждого шага спуска используйте ``np.clip`` к очередной точке ``x_n``.\n",
        "\n",
        "* Попробуйте сделать больше одного запуска. Затем из всех найденных результатов выберите минимальный.\n",
        "\n",
        "* Авторское решение использует параметры ``iters = 5000`` и ``lr = 0.05``\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfBgQXzxfKRk"
      },
      "source": [
        "На каждой итерации в теле функции вызывается метод `callback(x, f(x))`, где `x` --- это результат шага градиентного спуска.   \n",
        "Это нужно для отрисовки шагов алгоритма."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o-LAqEHARF1"
      },
      "source": [
        "def grad_descent_v1(f, deriv, x0=None, lr=0.1, iters=100, callback=None):\n",
        "    \"\"\"\n",
        "    Реализация градиентного спуска для функций с одним локальным минимумом,\n",
        "    совпадающим с глобальным. Все тесты будут иметь такую природу.\n",
        "    :param func: float -> float — функция\n",
        "    :param deriv: float -> float — её производная\n",
        "    :param x0: float — начальная точка\n",
        "    :param lr: float — learning rate\n",
        "    :param iters: int — количество итераций\n",
        "    :param callback: callable — функция логирования\n",
        "    \"\"\"\n",
        "\n",
        "    if x0 is None:\n",
        "        # Если точка не дана, сгенерируем случайную\n",
        "        # из стандартного нормального распределения.\n",
        "        # При таком подходе начальная точка может быть\n",
        "        # любой, а не только из какого-то ограниченного диапазона\n",
        "        np.random.seed(179)\n",
        "\n",
        "        x0 = np.random.uniform()\n",
        "\n",
        "    x = x0\n",
        "\n",
        "    callback(x, f(x))  # не забывайте логировать\n",
        "\n",
        "    while iters > 0 and deriv(x) not in (-1e-2, 0, 1e-2):\n",
        "        x -= lr * deriv(x)\n",
        "        iters -= 1\n",
        "        callback(x, f(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Is2v1J4dJy"
      },
      "source": [
        "### Отрисовка и тесты\n",
        "Рекомедуем пользоваться!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwZyc1vVd3h9"
      },
      "source": [
        "def plot_convergence_1d(func, x_steps, y_steps, ax, grid=None, title=\"\"):\n",
        "    \"\"\"\n",
        "    Функция отрисовки шагов градиентного спуска.\n",
        "    Не меняйте её код без необходимости!\n",
        "    :param func: функция, которая минимизируется градиентным спуском\n",
        "    :param x_steps: np.array(float) — шаги алгоритма по оси Ox\n",
        "    :param y_steps: np.array(float) — шаги алгоритма по оси Оу\n",
        "    :param ax: холст для отрисовки графика\n",
        "    :param grid: np.array(float) — точки отрисовки функции func\n",
        "    :param title: str — заголовок графика\n",
        "    \"\"\"\n",
        "    ax.set_title(title, fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "    if grid is None:\n",
        "        grid = np.linspace(np.min(x_steps), np.max(x_steps), 100)\n",
        "\n",
        "    fgrid = [func(item) for item in grid]\n",
        "    ax.plot(grid, fgrid)\n",
        "    yrange = np.max(fgrid) - np.min(fgrid)\n",
        "\n",
        "    arrow_kwargs = dict(linestyle=\"--\", color=\"grey\", alpha=0.4)\n",
        "    for i, _ in enumerate(x_steps):\n",
        "        if i + 1 < len(x_steps):\n",
        "            ax.arrow(\n",
        "                x_steps[i], y_steps[i],\n",
        "                x_steps[i + 1] - x_steps[i],\n",
        "                y_steps[i + 1] - y_steps[i],\n",
        "                **arrow_kwargs\n",
        "            )\n",
        "\n",
        "    n = len(x_steps)\n",
        "    color_list = [(i / n, 0, 0, 1 - i / n) for i in range(n)]\n",
        "    ax.scatter(x_steps, y_steps, c=color_list)\n",
        "    ax.scatter(x_steps[-1], y_steps[-1], c=\"red\")\n",
        "    ax.set_xlabel(r\"$x$\")\n",
        "    ax.set_ylabel(r\"$y$\")\n",
        "\n",
        "\n",
        "class LoggingCallback:\n",
        "    \"\"\"\n",
        "    Класс для логирования шагов градиентного спуска.\n",
        "    Сохраняет точку (x, f(x)) на каждом шаге.\n",
        "    Пример использования в коде: callback(x, f(x))\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.x_steps = []\n",
        "        self.y_steps = []\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        self.x_steps.append(x)\n",
        "        self.y_steps.append(y)\n",
        "\n",
        "\n",
        "def test_convergence_1d(grad_descent, test_cases, tol=1e-2, axes=None, grid=None):\n",
        "    \"\"\"\n",
        "    Функция для проверки корректности вашего решения в одномерном случае.\n",
        "    Она же используется в тестах на Stepik, так что не меняйте её код!\n",
        "    :param grad_descent: ваша реализация градиентного спуска\n",
        "    :param test_cases: dict(dict), тесты в формате dict с такими ключами:\n",
        "        - \"func\" — функция (обязательно)\n",
        "        - \"deriv\" — её производная (обязательно)\n",
        "        - \"start\" — начальная точка start (м.б. None) (опционально)\n",
        "        - \"low\", \"high\" — диапазон для выбора начальной точки (опционально)\n",
        "        - \"answer\" — ответ (обязательно)\n",
        "    При желании вы можете придумать и свои тесты.\n",
        "    :param tol: предельное допустимое отклонение найденного ответа от истинного\n",
        "    :param axes: матрица холстов для отрисовки, по ячейке на тест\n",
        "    :param grid: np.array(float), точки на оси Ох для отрисовки тестов\n",
        "    :return: флаг, корректно ли пройдены тесты, и дебажный вывод в случае неудачи\n",
        "    \"\"\"\n",
        "    right_flag = True\n",
        "    debug_log = []\n",
        "    for i, key in enumerate(test_cases.keys()):\n",
        "        # Формируем входные данные и ответ для алгоритма.\n",
        "        answer = test_cases[key][\"answer\"]\n",
        "        test_input = deepcopy(test_cases[key])\n",
        "        del test_input[\"answer\"]\n",
        "        # Запускаем сам алгоритм.\n",
        "        callback = LoggingCallback()  # Не забываем про логирование\n",
        "        res_point = grad_descent(*test_input.values(), callback=callback)\n",
        "        # Отрисовываем результаты.\n",
        "        if axes is not None:\n",
        "            ax = axes[np.unravel_index(i, shape=axes.shape)]\n",
        "            x_steps = np.array(callback.x_steps)\n",
        "            y_steps = np.array(callback.y_steps)\n",
        "            plot_convergence_1d(\n",
        "                test_input[\"func\"], x_steps, y_steps,\n",
        "                ax, grid, key\n",
        "            )\n",
        "            ax.axvline(answer, 0, linestyle=\"--\", c=\"red\",\n",
        "                        label=f\"true answer = {answer}\")\n",
        "            ax.axvline(res_point, 0, linestyle=\"--\", c=\"xkcd:tangerine\",\n",
        "                        label=f\"estimate = {np.round(res_point, 3)}\")\n",
        "            ax.legend(fontsize=16)\n",
        "        # Проверяем, что найдення точка достаточно близко к истинной\n",
        "        if abs(answer - res_point) > tol or np.isnan(res_point):\n",
        "            debug_log.append(\n",
        "                f\"Тест '{key}':\\n\"\n",
        "                f\"\\t- ответ: {answer}\\n\"\n",
        "                f\"\\t- вывод алгоритма: {res_point}\"\n",
        "            )\n",
        "            right_flag = False\n",
        "    return right_flag, debug_log"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eOO6t0cvoeh",
        "outputId": "295938bb-76db-469c-dd57-190e83699aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_cases = {\n",
        "    \"square\": {\n",
        "        \"func\" : lambda x: x * x,\n",
        "        \"deriv\" : lambda x: 2 * x,\n",
        "        \"start\" : 2,\n",
        "        \"answer\" : 0.0\n",
        "    },\n",
        "    \"module\": {\n",
        "        \"func\" : lambda x: abs(x),\n",
        "        \"deriv\" : lambda x: 1 if x > 0 else -1,\n",
        "        \"start\" : 2,\n",
        "        \"answer\" : 0.0\n",
        "    },\n",
        "    \"third_power\": {\n",
        "        \"func\" : lambda x: abs((x - 1)**3),\n",
        "        \"deriv\" : lambda x: 3 * (x - 1)**2 * np.sign(x-1),\n",
        "        \"start\" : -1,\n",
        "        \"answer\" : 1.0\n",
        "    },\n",
        "    \"ln_x2_1\": {\n",
        "        \"func\" : lambda x: np.log((x + 1)**2 + 1),\n",
        "        \"deriv\" : lambda x: 2 * (x + 1) / (x**2 +1),\n",
        "        \"start\" : 1,\n",
        "        \"answer\" : -1.0\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "tol = 1e-2  # желаемая точность\n",
        "fig, axes = plt.subplots(2,2, figsize=(14, 12))\n",
        "fig.suptitle(\"Градиентный спуск, версия 1\", fontweight=\"bold\", fontsize=20)\n",
        "grid = np.linspace(-2, 2, 100)\n",
        "\n",
        "is_correct, debug_log = test_convergence_1d(\n",
        "    grad_descent_v1, test_cases, tol,\n",
        "    axes, grid\n",
        ")\n",
        "if not is_correct:\n",
        "    print(\"Не сошлось. Дебажный вывод:\")\n",
        "    for log_entry in debug_log:\n",
        "        print(log_entry)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-83b92d628999>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m is_correct, debug_log = test_convergence_1d(\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mgrad_descent_v1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9f4709dc7bb2>\u001b[0m in \u001b[0;36mtest_convergence_1d\u001b[0;34m(grad_descent, test_cases, tol, axes, grid)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Запускаем сам алгоритм.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoggingCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Не забываем про логирование\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mres_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Отрисовываем результаты.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-d46f54df6c38>\u001b[0m in \u001b[0;36mgrad_descent_v1\u001b[0;34m(f, deriv, x0, lr, iters, callback)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0miters\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mderiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mderiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0miters\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \"\"\"\n\u001b[0;32m-> 2247\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmax\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"One of max or min must be given\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1200 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pl7exWe3YLd"
      },
      "source": [
        "​\n",
        "# Основные положения дифференциального исчисления функций многих переменных\n",
        "\n",
        "Если вдруг Вы не знаете или не помните дифференциальное исчисление функций многих переменных, то вот несколько ключевых определений, которые помогут Вам справиться с заданиями 7-9. Здесь и ниже рассматриваются скалярные функции многих переменных, т.е. $f:U\\rightarrow \\mathbb{R}$, где $U$ --- область в $\\mathbb{R}^n$.\n",
        "\n",
        "\n",
        "Мы хотим ввести некоторый аналог производной. Что мы можем делать уже сейчас --- это вычислять производные функции многих переменных по отдельным аргументам.\n",
        "\n",
        "## Дифференцируемость функции многих переменных\n",
        "**Определение**.* Частной производной функции нескольких переменных* $f(x_1,x_2,\\ldots, x_n)$ по аргументу $x_i$ в точке $\\overline{x^0} = (x_1^0, x_2^0, \\ldots, x_n^0)$ называется производная функции $f$ по $x_i$ в точке $x^0_i$ как функции одного аргумента при фиксированных значениях $x_1^0$, $x_2^0, \\ldots$, $x_{i-1}^0$, $x_{i+1}^0, \\ldots$, $x_n^0$. Иными словами, частная производная равна вот такому пределу:\n",
        "\n",
        "$$\\lim_{h \\to 0}\\frac{f(x_1,\\ldots,x_{i-1}, x_i + h, x_{i+1}, \\ldots, x_n) - f(x_1,\\ldots,x_n)}{h}.$$\n",
        "\n",
        "**Обозначение**:\n",
        "\n",
        "$$f'_{x_i}(\\overline{x^0}); \\quad \\frac{\\partial f}{\\partial x_i}(\\overline{x_0}).$$\n",
        "\n",
        "**Определение**. Функция $f: \\mathbb{R}^n \\to \\mathbb{R}$ называется *дифференцируемой в точке* $\\overline{x^0} = (x_1^0, x_2^0, \\ldots, x_n^0)$, если имеет место представление\n",
        "$$f(\\overline{x^0} + \\overline{\\Delta x}) = f(\\overline{x^0}) + \\langle \\overline{a}, \\overline{\\Delta x} \\rangle + o(|\\overline{\\Delta x}|),$$ где $\\overline{a} \\in \\mathbb{R}^n$ --- некоторый $n$-мерный вектор, который называется градиентом функции $f$ в точке $\\overline{x^0}$.\n",
        "\n",
        "**Обозначения для градиента**:\n",
        "$$\\mathrm{grad} f(\\overline{x^0}) = \\nabla f(\\overline{x^0}) = f'(\\overline{x^0}).$$\n",
        "\n",
        "## Связь градиента и частных производных\n",
        "Как можно заметить, определение выше, во-первых, полностью аналогично свойству производной функции одной переменной. Во-вторых, это определение довольно бесполезно. Оказывается, во всех \"хороших\" случаях справедливо следующее утверждение.\n",
        "\n",
        "**Теорема**.\n",
        "Пусть функция $f$ имеет в точке $\\overline{x^0}$ непрерывные частные производные по каждой компоненте $x_i$. Тогда $f$ дифференцируема в точке $\\overline{x^0}$, причём её градиент равен вектору из частных производных, то есть\n",
        "$$\\left(\\frac{\\partial f}{\\partial x_1}(\\overline{x^0}), \\frac{\\partial f}{\\partial x_2}(\\overline{x^0}), \\ldots, \\frac{\\partial f}{\\partial x_n}(\\overline{x^0})\\right).$$\n",
        "\n",
        "Суть этой теоремы заключается в том, что во всех ``хороших'' случаях градиент существует и его очень просто вычислить --- нужно просто посчитать частные производные по всем переменным.\n",
        "\n",
        "**Замечание**.\n",
        "Градиент указывает на направление наискорейшего роста значения функции. Иными словами, при движении точки, стартующей в $\\overline{x^0}$, по вектору $\\mathrm{grad} f(\\overline{x^0})$, значение функции увеличивается.\n",
        "\n",
        "\n",
        "\n",
        "## Алгоритм градиентного спуска\n",
        "Алгоритм градиентного спуска для поиска минимума функции $n$ переменных $f(x_1,x_2,\\ldots, x_n)$ состоит в итеративном поиске точки минимума функции по формуле для $k+1$-ой точки через $k$-ую точку и градиент в $k$-ой точке.\n",
        "\n",
        "$$\\overline{x}^{k+1} = \\overline{x}^k - \\lambda\\nabla f(\\overline{x}^k),$$\n",
        "\n",
        "где $\\lambda$ --- положительное число, называемое learning rate. Обратите внимание, что здесь верхние индексы обозначают не степень, а номер точки в последовательности! То есть под $\\overline{x}^k$ понимается вектор $(x^k_1,\\ldots, x^k_n)$ --- $k$-ая точка из последовательности приближений, которую строит алгоритм.\n",
        "\n",
        "​"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-8n8ZzIvoe_"
      },
      "source": [
        "## Задание 6\n",
        "Вычислите градиент следующей функции:\n",
        "\n",
        "$$\\psi(x,y,z) = sin(xz) - y^2z + e^x$$\n",
        "\n",
        "Для того, чтобы проверить ответ, заполните пропуски в коде ниже. Правильный ответ должен проходить все assert'ы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q47B6rkvofB"
      },
      "source": [
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4uFHoIfsV_0"
      },
      "source": [
        "from math import sin, cos, tan, exp, sqrt, pi\n",
        "import numpy as np\n",
        "\n",
        "def grad_1(x, y, z):\n",
        "    #возвращает кортеж из 3 чисел --- частных производных по x,y,z\n",
        "\n",
        "    dx = #YOUR CODE\n",
        "    dy = #YOUR CODE\n",
        "    dz = #YOUR CODE\n",
        "    return (dx, dy, dz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKK81eg_ikMr"
      },
      "source": [
        "#Тестируем нашу функцию\n",
        "import numpy as np\n",
        "\n",
        "assert np.allclose(grad_1(1,1,1), (3.258584134327185, -2, -0.45969769413186023), atol=5e-6)\n",
        "assert np.allclose(grad_1(1, 8, 0), (2.718281828459045, 0, -63.0), atol=5e-6)\n",
        "assert np.allclose(grad_1(-11,pi,1), (0.004442399688841031, -6.283185307179586, -9.918287078957917), atol=5e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ3JJG_mvofD"
      },
      "source": [
        "## Задание 7 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn9o18L5vofE"
      },
      "source": [
        "Еще один градиент:\n",
        "\n",
        "$\\psi(x,y,z) = \\ln(\\cos(e^{x+y})) - \\ln(xy)$\n",
        "\n",
        "Заполните пропуски в функции ниже.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hir9sXiUVPC4"
      },
      "source": [
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYcDdBZ2-BaT"
      },
      "source": [
        "def grad_2(x, y, z):\n",
        "    #возвращает кортеж из 3 чисел --- частных производных по x,y,z\n",
        "    dx = #YOUR CODE\n",
        "    dy = #YOUR CODE\n",
        "    dz = #YOUR CODE\n",
        "    return (dx, dy, dz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAjyySeKi8YN"
      },
      "source": [
        "#Тестируем нашу функцию\n",
        "\n",
        "assert np.allclose(grad_2(1,1,0), (-15.73101919885423, -15.73101919885423, 0), atol=5e-6)\n",
        "assert np.allclose(grad_2(-10, 3, 0), (0.09999916847105042, -0.3333341648622829, 0), atol=5e-6)\n",
        "assert np.allclose(grad_2(15 ,4, 0), (54654806.79650013, 54654806.6131668,0), atol=5e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJi66t39voey"
      },
      "source": [
        "## Задание 8 ##\n",
        "А теперь все вместе!\n",
        "\n",
        "У вас есть только функция, которую Вам отдают в качестве аргумента и вы должны найти её минимум.\n",
        "\n",
        "Вы будете искать глобальный, у вас это должно получиться лишь потому, что тут они хорошие.\n",
        "\n",
        "Да, и еще, теперь они не одномерные, а двумерные. Также вам будут даны начальные точки, сходимость из которых гарантируется.\n",
        "\n",
        "***Подсказка***. Можете использовать следующие параметры:\n",
        "\n",
        "* Отклонение при вычислении производной $\\varepsilon = 10^{-10}$\n",
        "* Критерий остановки: кол-во итераций $10^4$\n",
        "* Длина шага градиентного спуска $lr = 0.5$\n",
        "\n",
        "\n",
        "И вновь мы предоставляем функцию отрисовки шагов для пущего удобства."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egm2nGHCvoe1"
      },
      "source": [
        "def numerical_derivative_2d(func, epsilon):\n",
        "    \"\"\"\n",
        "    Функция для приближённого вычисления градиента функции двух переменных.\n",
        "    :param func: np.array[2] -> float — произвольная дифференцируемая функция\n",
        "    :param epsilon: float — максимальная величина приращения по осям\n",
        "    :return: другая функция, которая приближённо вычисляет градиент в точке\n",
        "    \"\"\"\n",
        "    def grad_func(x):\n",
        "        \"\"\"\n",
        "        :param x: np.array[2] — точка, в которой нужно вычислить градиент\n",
        "        :return: np.array[2] — приближённое значение градиента в этой точке\n",
        "        \"\"\"\n",
        "\n",
        "        return np.array([\n",
        "            (func(x + np.array([epsilon, 0])) - func(x)) / epsilon,\n",
        "            (func(x + np.array([0, epsilon])) - func(x)) / epsilon\n",
        "        ])\n",
        "\n",
        "    return grad_func\n",
        "\n",
        "\n",
        "def grad_descent_2d(func, low, high, start=None, callback=None, lr=0.5, iter=10**4):\n",
        "    \"\"\"\n",
        "    Реализация градиентного спуска для функций двух переменных\n",
        "\n",
        "    Обратите внимание, что здесь градиент функции не дан.\n",
        "    Его нужно вычислять приближённо.\n",
        "\n",
        "    :param func: np.ndarray -> float — функция\n",
        "    \"\"\"\n",
        "\n",
        "    # Инициализируем start случайными числами,\n",
        "    # если стартовая точка не дана\n",
        "    if start is None:\n",
        "        start = np.random.randn(2)\n",
        "\n",
        "    eps = 1e-10\n",
        "    df = numerical_derivative_2d(func, eps)\n",
        "\n",
        "    while iter > 0:\n",
        "        iter -= 1\n",
        "        start[0] -= lr * df(start)[0]\n",
        "        start[1] -= lr * df(start)[1]\n",
        "        callback(start, func(start))\n",
        "\n",
        "    return start"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARnTlYWcLGo0"
      },
      "source": [
        "def plot_convergence_2d(func, steps, ax, xlim, ylim, cmap=\"viridis\", title=\"\"):\n",
        "    \"\"\"\n",
        "    Функция отрисовки шагов градиентного спуска.\n",
        "    Не меняйте её код без необходимости!\n",
        "    :param func: функция, которая минимизируется градиентным спуском\n",
        "    :param steps: np.array[N x 2] — шаги алгоритма\n",
        "    :param ax: холст для отрисовки графика\n",
        "    :param xlim: tuple(float), 2 — диапазон по первой оси\n",
        "    :param ylim: tuple(float), 2 — диапазон по второй оси\n",
        "    :param cmap: str — название палитры\n",
        "    :param title: str — заголовок графика\n",
        "    \"\"\"\n",
        "\n",
        "    ax.set_title(title, fontsize=20, fontweight=\"bold\")\n",
        "    # Отрисовка значений функции на фоне\n",
        "    xrange = np.linspace(*xlim, 100)\n",
        "    yrange = np.linspace(*ylim, 100)\n",
        "    grid = np.meshgrid(xrange, yrange)\n",
        "    X, Y = grid\n",
        "    fvalues = func(\n",
        "        np.dstack(grid).reshape(-1, 2)\n",
        "    ).reshape((xrange.size, yrange.size))\n",
        "    ax.pcolormesh(xrange, yrange, fvalues, cmap=cmap, alpha=0.8)\n",
        "    CS = ax.contour(xrange, yrange, fvalues)\n",
        "    ax.clabel(CS, CS.levels, inline=True)\n",
        "    # Отрисовка шагов алгоритма в виде стрелочек\n",
        "    arrow_kwargs = dict(linestyle=\"--\", color=\"black\", alpha=0.8)\n",
        "    for i, _ in enumerate(steps):\n",
        "        if i + 1 < len(steps):\n",
        "            ax.arrow(\n",
        "                *steps[i],\n",
        "                *(steps[i+1] - steps[i]),\n",
        "                **arrow_kwargs\n",
        "            )\n",
        "    # Отрисовка шагов алгоритма в виде точек\n",
        "    n = len(steps)\n",
        "    color_list = [(i / n, 0, 0, 1 - i / n) for i in range(n)]\n",
        "    ax.scatter(steps[:, 0], steps[:, 1], c=color_list, zorder=10)\n",
        "    ax.scatter(steps[-1, 0], steps[-1, 1],\n",
        "               color=\"red\", label=f\"estimate = {np.round(steps[-1], 2)}\")\n",
        "    # Финальное оформление графиков\n",
        "    ax.set_xlim(xlim) #добавления low и high в\n",
        "    ax.set_ylim(ylim)\n",
        "    ax.set_ylabel(\"$y$\")\n",
        "    ax.set_xlabel(\"$x$\")\n",
        "    ax.legend(fontsize=16)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFgjqiYZvoe5"
      },
      "source": [
        "def test_convergence_2d(grad_descent_2d, test_cases, tol, axes=None):\n",
        "    \"\"\"\n",
        "    Функция для проверки корректности вашего решения в двумерном случае.\n",
        "    Она же используется в тестах на Stepik, так что не меняйте её код!\n",
        "    :param grad_descent_2d: ваша реализация градиентного спуска\n",
        "    :param test_cases: dict(dict), тесты в формате dict с такими ключами:\n",
        "        - \"func\" — функция\n",
        "        - \"deriv\" — её производная\n",
        "        - \"low\", \"high\" — диапазон для выбора начальной точки\n",
        "        - \"answer\" — ответ\n",
        "    При желании вы можете придумать и свои тесты.\n",
        "    :param tol: предельное допустимое отклонение найденного ответа от истинного\n",
        "    :param axes: матрица холстов для отрисовки, по ячейке на тест\n",
        "    :return: флаг, корректно ли пройдены тесты, и дебажный вывод в случае неудачи\n",
        "    \"\"\"\n",
        "    right_flag = True\n",
        "    debug_log = []\n",
        "    for i, key in enumerate(test_cases.keys()):\n",
        "        # Формируем входные данные и ответ для алгоритма.\n",
        "        answer = test_cases[key][\"answer\"]\n",
        "        test_input = deepcopy(test_cases[key])\n",
        "        del test_input[\"answer\"]\n",
        "        # Запускаем сам алгоритм.\n",
        "        callback = LoggingCallback()  # Не забываем про логирование\n",
        "        res_point = grad_descent_2d(**test_input, callback=callback)\n",
        "        # Отрисовываем результаты.\n",
        "        if axes is not None:\n",
        "            ax = axes[np.unravel_index(i, shape=axes.shape)]\n",
        "            plot_convergence_2d(\n",
        "                np.vectorize(test_input[\"func\"], signature=\"(n)->()\"),\n",
        "                np.vstack(callback.x_steps),\n",
        "                ax=ax,\n",
        "                xlim=(test_input[\"low\"], test_input[\"high\"]),\n",
        "                ylim=(test_input[\"low\"], test_input[\"high\"]),\n",
        "                title=key\n",
        "            )\n",
        "        # Проверяем, что найденная точка достаточно близко к истинной\n",
        "        if np.linalg.norm(answer - res_point, ord=1) > tol:\n",
        "            debug_log.append(\n",
        "                f\"Тест '{key}':\\n\"\n",
        "                f\"\\t- ответ: {answer}\\n\"\n",
        "                f\"\\t- вывод алгоритма: {res_point}\"\n",
        "            )\n",
        "            right_flag = False\n",
        "    return right_flag, debug_log\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR_-Ucs4giUQ"
      },
      "source": [
        "test_cases = {\n",
        "    \"concentric_circles\" : {\n",
        "        \"func\" : lambda x: (\n",
        "            -1 / ((x[0] - 1)**2 + (x[1] - 1.5)**2 + 1)\n",
        "            * np.cos(2 * (x[0] - 1)**2 + 2 * (x[1] - 1.5)**2)\n",
        "        ),\n",
        "        \"low\" : -5,\n",
        "        \"high\" : 5,\n",
        "        \"start\": np.array([.2 , .7]),\n",
        "        \"answer\" : np.array([1, 1.5])\n",
        "    },\n",
        "        \"other concentric circles\" : {\n",
        "       \"func\" : lambda x: (\n",
        "            -1 / ((x[0])**2 + (x[1] - 3)**2 + 1)\n",
        "            * np.cos(2 * (x[0])**2 + 2 * (x[1] - 3)**2)\n",
        "        ),\n",
        "        \"low\" : -5,\n",
        "        \"high\" : 5,\n",
        "        \"start\": np.array([1.1, 3.3]),\n",
        "        \"answer\" : np.array([0, 3])\n",
        "    },\n",
        "    \"straightened ellipses\" : {\n",
        "        \"func\" : lambda x: (\n",
        "            -1 / ((x[0] )**4 + (x[1] - 3)**6 + 1)\n",
        "            * np.cos(2 * (x[0])**4 + 2 * (x[1] - 3)**6)\n",
        "        ),\n",
        "        \"low\" : -5,\n",
        "        \"high\" : 5,\n",
        "        \"start\": np.array([.8, 3.001]), # точка так близко к ответу тк в окрестности ответа градиент маленкьий и функция очень плохо сходится\n",
        "        \"answer\" : np.array([0, 3])\n",
        "    },\n",
        "}\n",
        "tol = 1e-2  # желаемая точность\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(30, 10), squeeze=False)\n",
        "fig.suptitle(\"Градиентный спуск в 2D\", fontsize=25, fontweight=\"bold\")\n",
        "is_correct, debug_log = test_convergence_2d(grad_descent_2d, test_cases, tol, axes)\n",
        "\n",
        "if not is_correct:\n",
        "    print(\"Не сошлось. Дебажный вывод:\")\n",
        "    for log_entry in debug_log:\n",
        "        print(log_entry)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBGT01Idqaod"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}